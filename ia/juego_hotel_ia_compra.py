# -*- coding: utf-8 -*-
"""juego_hotel-IA_Compra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rXkRiRnq59435RtOSICPrA1T2dXyxG6
"""

import pathlib
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("Mil_Partidas_Hotel.csv")
df

#array([0, 5, 7, 2, 4, 6, 3, 1, 8])

df['propiedad_compra'].unique()

df.info()

df.describe(include='all')

df['valor_casilla'].unique()

df.hist(figsize=(20,10), color = 'teal')

df.isna().sum()

df = df.replace(np.nan, 0, regex=True)

df.isna().sum()

df['compra'] = df['compra'].replace(0, 'no', regex=True)
df['construye'] = df['construye'].replace(0, 'no', regex=True)

df['propiedad_compra'].unique()

columns = df.columns[2:-3]

lb = LabelEncoder()
for atrib in columns:
  df[atrib] = lb.fit_transform(df[atrib].astype(str))
df['ganador'] = lb.fit_transform(df['ganador'].astype(str))
df

df['valor_casilla'].unique()

corr_matrix = df.corr()
import seaborn as sns

fig, ax = plt.subplots(figsize=(20, 20))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".1f",
                 cmap="plasma");
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)

#sns.pairplot(df,diag_kind="kde",corner=True,
#             markers="+",
#             plot_kws=dict(s=1, edgecolor="b", linewidth=1),
#             diag_kws=dict(shade=True) )

sns.heatmap(df.corr()[['compra']].sort_values('compra', ascending=False), vmin=-1, vmax=1, annot=True)

sns.heatmap(df.corr()[['construye']].sort_values('construye', ascending=False), vmin=-1, vmax=1, annot=True)

# Putting feature variable to X
#X = df.drop(["compra", "operacion_jugador", 'propiedad_compra'], axis=1)
X = df.filter(['tirada', 'casilla', 'valor_casilla']) # para compra

#X = df.drop("construye", axis=1)
#X = df.filter(['dinero', 'casilla', 'valor_casilla', 'operacion_jugador', 'pago_noches']) # para construye

# Putting response variable to y
y = df['propiedad_compra']

#X = (X-np.min(X))/(np.max(X)-np.min(X))

from sklearn.model_selection import train_test_split
# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=50)
X_train.shape

"""# Entrenamiento del modelo

## AdaBoost
"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# Create adaboost classifer object
abc = AdaBoostClassifier(n_estimators=50,
                         learning_rate=1) # base_estimator -> If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.
# Train Adaboost Classifer
model = abc.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = model.predict(X_train)

print("Accuracy:", accuracy_score(y_train, y_pred))

"""## GridSearch para Adaboostclassifier"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

param_dist = {
}

model = GridSearchCV(estimator = AdaBoostClassifier(), param_grid = param_dist, cv = 7)

model.fit(X_train, y_train)


mejor_estim = model.best_estimator_
mejor_param = model.best_params_
mejor_score = model.best_score_

print("\n----------------------------------------------------------------------")
print("Mejor estimador              :", mejor_estim)
print("Mejor valores parametrización:", mejor_param)
print("Mejor puntuación             :", mejor_score)
print("----------------------------------------------------------------------\n")

"""## Naive Bayes - (El mejor)"""

from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB

# 0. Creación de instancia del modelo
# Con asignación de valores a hiperparámetros
model = GaussianNB(priors = None, var_smoothing = 1e-10)

#2. Entrenamiento del modelo: fit()
model = model.fit(X_train, y_train)

#3. Predicción sobre los datos de entrenamiento.
y_pred = model.predict(X_train)

#4. Métricas para calidad de resultados de clasificación con datos de prueba.
print("Accuracy:",accuracy_score(y_train, y_pred))

"""## GridSearch para Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

param_dist = {
    "var_smoothing": [1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5],
    "priors": [0.1, 0.9]
}

model = GridSearchCV(estimator = GaussianNB(), param_grid = param_dist, cv = 3)

model = model.fit(X_train, y_train)

mejor_estim = model.best_estimator_
mejor_param = model.best_params_
mejor_score = model.best_score_

print("\n----------------------------------------------------------------------")
print("Mejor estimador              :", mejor_estim)
print("Mejor valores parametrización:", mejor_param)
print("Mejor puntuación             :", mejor_score)
print("----------------------------------------------------------------------\n")

import pydot
from IPython.display import Image
from sklearn import tree
pd.DataFrame(
{'Atributos': X_train.columns,
 'Relevancia': mejor_estim.feature_importances_
}).plot(x='Atributos', y='Relevancia', kind='bar')

"""## DecisionTree

"""

from sklearn.tree import DecisionTreeClassifier

# Create Decision Tree classifer object
model = DecisionTreeClassifier()

# Train Decision Tree Classifer
model = model.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = model.predict(X_train)

print("Accuracy:",accuracy_score(y_pred, y_train))
print(classification_report(y_train, y_pred))

"""## RandomForest"""

#Usamos GridSearch para encontrar el mejor valor para los hiperparámetros del método RF
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Definimos el rango para los parámetros
max_depth=[2, 8, 16]
n_estimators = [64, 128, 256]
param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)

# construimos el grid de búsqueda
dfrst = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
grid = GridSearchCV(estimator=dfrst, param_grid=param_grid, cv = 5)
grid_results = grid.fit(X_train, y_train)

# Resumen de los resultados obtenidos
print("Best: {0}, using {1}".format(grid_results.cv_results_['mean_test_score'], grid_results.best_params_))
results_df = pd.DataFrame(grid_results.cv_results_)
results_df

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

model = RandomForestClassifier(max_depth=2, n_estimators=64, random_state=0)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

# Model Accuracy, how often is the classifier correct?
print("\nAccuracy:",accuracy_score(y_train, y_pred))
print(classification_report(y_train, y_pred))

"""## Xgboost"""

# Establecer un baseline
import xgboost as xgb
from sklearn.metrics import accuracy_score

model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
model = model.fit(X_train, y_train)

Y_pred_xgb = model.predict(X_test)
score_xgb = round(accuracy_score(Y_pred_xgb,y_test)*100,2)

print("The accuracy score achieved using XGBoost is: "+str(score_xgb)+" %")

"""## Modelo 2"""

'''
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.callbacks import EarlyStopping

X_train, Y_train = np.array(X_train), np.array(y_train)

# Reshape X_train para que se ajuste al modelo en Keras
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

#
# Red LSTM
#
dim_entrada = (X_train.shape[1],1)
na = 150

model = Sequential()
model.add(LSTM(units=32, input_shape=dim_entrada))
model.add(Dense(units=64))
model.add(Dense(units=1, activation='sigmoid'))
#model.compile(optimizer='rmsprop', loss='mse',  metrics=['mae', 'mse'])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

earlyStop=EarlyStopping(monitor="val_loss",verbose=2,mode='min',patience=9)
history=model.fit(X_train,Y_train,epochs=100,batch_size=10,validation_data=(X_test,y_test) ,verbose=1,callbacks=[earlyStop])
'''

"""## Modelo 3"""

'''
X_train, Y_train = np.array(X_train), np.array(y_train)

# Reshape X_train para que se ajuste al modelo en Keras
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

EPOCHS = 25

model = keras.Sequential([
  layers.Dense(32, activation='relu', input_shape=[X_train.shape[1]], activity_regularizer=tf.keras.regularizers.l1(0.001)),
  layers.Dense(32, activation='relu'),
  layers.Dense(1)
])

optimizer = tf.keras.optimizers.RMSprop(0.001)

model.compile(loss='mse',
              optimizer=optimizer,
              metrics=['mae', 'mse'])

model.summary()

history = model.fit(X_train, Y_train, epochs=EPOCHS, validation_split = 0.2, verbose=1)
'''

"""## Modelo 4"""

'''
#X_train, Y_train = np.array(X_train), np.array(y_train)

# Reshape X_train para que se ajuste al modelo en Keras
#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

INPUT_DIM = X_test.shape[1]

HL1_DIM = 62
HL2_DIM = 30

model = tf.keras.models.Sequential()

model.add(layers.Dense(HL1_DIM, activation='relu', input_dim = INPUT_DIM))
model.add(layers.Dense(HL2_DIM, activation='relu', kernel_initializer='he_uniform'))
model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.GlorotUniform()))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])

history = model.fit(X_train, y_train, batch_size=10, epochs=5)
'''

"""## Resultados entrenamiento"""

import pydot
from IPython.display import Image
from sklearn import tree
pd.DataFrame(
{'Atributos': X_train.columns,
 'Relevancia': abc.feature_importances_
}).sort_values(by=['Relevancia'], ascending=False).plot(x='Atributos', y='Relevancia', kind='bar')

y_pred = model.predict(X_test)
y_pred.shape, y_test.shape

accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

import sklearn.metrics as metrics
# calculate the fpr and tpr for all thresholds of the classification
probs = model.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

est_predictions = model.predict(X_test).flatten()
plt.scatter(y_test, est_predictions)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.axis('equal')
plt.axis('square')
plt.xlim([0,plt.xlim()[1]])
plt.ylim([0,plt.ylim()[1]])
_ = plt.plot([-100, 100], [-100, 100])

error = est_predictions - y_test
plt.hist(error, bins = 25)
plt.xlabel("Prediction Error")
_ = plt.ylabel("Count")

"""## Predicción"""

# Necesario solo para Xgboost
'''
names = model.get_booster().feature_names

result = model.predict(X_test[names].iloc[[219]])

result
'''

"""## Predicción individual"""

X_test.iloc[[10]], y_test.iloc[[10]]

lista_propiedades = ['no compra', 'Al Walid', 'Artika', 'Coconut', 'Dragon gate', 'Reef Resort', 'Reine', 'Up Town towers', 'Zebra Lodge']

# 'tirada', 'casilla', 'valor_casilla'
result = model.predict(X_test.iloc[[11]])
print(str(result[0]) + ", " + str(y_test.iat[11]))

lista_propiedades[result[0]], lista_propiedades[y_test.iat[11]]

from tqdm.notebook import tqdm as tqdm

total=0
for i in range(len(y_test)):
  if y_test.iloc[i]:
    #print(i)
    total +=1

err = 0
for i in tqdm(range(len(X_test))):
  #result = model.predict(X_test[names].iloc[[i]]) # Para XgBoost
  result = model.predict(X_test.iloc[[i]])
  #print(result > 0.5, y_test.iloc[i])
  if y_test.iloc[i] != 0 and result[0] != y_test.iloc[i]:
    #print("Predicción: " + str(result) + " Resultado : " + str(y_test.iloc[i]))
    err += 1

print("Error de predicción en compra: " +  str(err) + "/" + str(total) + " (" +str(round(err * 100/ total)) + "%)")

from tqdm.notebook import tqdm as tqdm

total=0
for i in range(len(y_test)):
    total +=1

aciertos = 0
for i in tqdm(range(len(X_test))):
  result = model.predict(X_test.iloc[[i]])
  if result[0] == y_test.iloc[i]:
    aciertos += 1

print("Aciertos de predicción en compra: " +  str(aciertos) + "/" + str(total) + " (" +str(round(aciertos * 100/ total)) + "%)")

"""# Exportación del modelo a JS

## Para modelos NO Keras
"""

#https://blog.cambridgespark.com/deploying-a-machine-learning-model-to-the-web-725688b851c7

import pickle

with open('hotel_compra.pkl', 'wb') as file:
  pickle.dump(model, file)

"""## Para modelos Keras"""

#Exportar el modelo en formato h5
model.save('hotel_compra.h5')

#Colab es Linux. Listemos el contenido de la carpeta actual para ver que se exporto el modelo
!ls

#Para convertirlo a tensorflow.js, primero debemos instalar la libreria
!pip install tensorflowjs

#Crear carpeta donde se colocaran los archivos resultantes
!mkdir modelo_compilado_js

#Realizar la exportacion a la carpeta de salida
!tensorflowjs_converter --input_format keras hotel_compra.h5 modelo_compilado_js

#Confirmar que en la carpeta de salida se hayan generado los archivos. Deben aparecer archivos "bin" y "json"
!ls modelo_compilado_js

#Para descargarlos, da clic del lado izquierdo en el icono de la carpeta
#y expande carpeta_salida. En los archivos utiliza los 3 puntos para descargarlos